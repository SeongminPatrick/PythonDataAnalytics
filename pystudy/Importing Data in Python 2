-----------------------------------------------------------
Importing Data in Python 2
-----------------------------------------------------------
1. Importing data from the Internet
-----------------------------------------------------------
Importing flat files from the web


from urllib.request import urlretrieve
import pandas as pd

# Assign url of file (red wine)
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())

# white wine
# url = 'http://archive.ics.uci.edu/ml/machine-learningdatabases/wine-quality/winequality-white.csv'

import matplotlib.pyplot as plt

# Plot first column of df
pd.DataFrame.hist(df.ix[:, 0:1])
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()

-----------------------------------------------------------
Importing non-flat files from the web - pd.read_excel


# Assign url of file
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file
xl = pd.read_excel(url, sheetname=None)  # all sheets

# Print the sheetnames to the shell
print(xl.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xl['1700'].head())

-----------------------------------------------------------
HTTP requests - urllib


# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "http://www.datacamp.com/teach/documentation"

# This packages the request
request = Request(url)

# Sends the request and catches the response
response = urlopen(request)

print(type(response))

# Extract the response
html = response.read()

# Be polite and close the response!
response.close()

-----------------------------------------------------------
HTTP requests - requests


# Import package
import requests

# Specify the url
url = "http://www.datacamp.com/teach/documentation"

# Packages the request, send the request and catch the response
r = requests.get(url)

# Extract the response
text = r.text

# Print the html
print(text)

-----------------------------------------------------------
Parsing HTML with BeautifulSoup


# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response
r = requests.get(url)

# Extracts the response as html
html_doc = r.text

# Create a BeautifulSoup object from the HTML
soup = BeautifulSoup(html_doc)

# Prettify the BeautifulSoup object
pretty_soup = soup.prettify()

# Print the response
print(pretty_soup)

-----------------------------------------------------------
Turning a webpage into data - getting the text


import requests
from bs4 import BeautifulSoup

url = 'https://www.python.org/~guido/'

r = requests.get(url)

html_doc = r.text

# Create a BeautifulSoup object from the HTML
soup = BeautifulSoup(html_doc)

# Get the title of Guido's webpage
guido_title = soup.title
print(guido_title)

# Get Guido's text
guido_text = soup.get_text()
print(guido_text)

-----------------------------------------------------------
getting the hyperlinks


import requests
from bs4 import BeautifulSoup

url = 'https://www.python.org/~guido/'
r = requests.get(url)

html_doc = r.text

# create a BeautifulSoup object from the HTML
soup = BeautifulSoup(html_doc)
print(soup.title)

# Find all 'a' tags (which define hyperlinks)
a_tags = soup.find_all('a')

# Print the URLs to the shell
for link in a_tags:
    print(link.get('href'))



-----------------------------------------------------------
2. Interacting with APIs to import data from the web
-----------------------------------------------------------
Loading and exploring a JSON


# Load JSON
with open("a_movie.json") as json_file:
    json_data = json.load(json_file)    # dictionary

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])

-----------------------------------------------------------
API requests

# pull some movie data down from the Open Movie Database (OMDB) using their API

import requests

url = 'http://www.omdbapi.com/?t=social+network'
r = requests.get(url)

print(r.text)

# Decode the JSON data into a dictionary
json_data = r.json()

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])

-----------------------------------------------------------
Wikipedia API


import requests

url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'
r = requests.get(url)

# Decode the JSON data into a dictionary
json_data = r.json()

# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)



-----------------------------------------------------------
3. Diving deep into the Twitter API
-----------------------------------------------------------
Twitter API Authentication


import tweepy

# Store OAuth authentication credentials in relevant variables
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"

# Pass OAuth details to tweepy's OAuth handler
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

-----------------------------------------------------------
Streaming tweets


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


-----------------------------------------------------------


