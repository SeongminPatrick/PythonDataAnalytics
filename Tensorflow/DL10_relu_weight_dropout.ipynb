{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU, Weight, Dropout & Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep and wide NN : ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inpuyt : x1, x2\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "# 2-5-4-1 network\n",
    "W1 = tf.Variable(tf.random_uniform([2,5], -1.0, 1.0))  \n",
    "W2 = tf.Variable(tf.random_uniform([5,4], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform([4,1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name=\"bias1\")\n",
    "b2 = tf.Variable(tf.zeros([4]), name=\"bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"bias3\")\n",
    "\n",
    "# hypothesis\n",
    "L2 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "L3 = tf.sigmoid(tf.matmul(L2, W2) + b2)\n",
    "hypothesis = tf.sigmoid(tf.matmul(L3, W3) + b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid 단점 : hidden layer가 깊어지면 backpropagation에서 vanishing gradient 현상이 나타나서 학습이 제대로 되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function을 sigmoid 대신 ReLU 사용하면 이런 현상을 피할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights in a smart way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not all 0's\n",
    "* Restricted Boatman Machine (RBM) : complicated\n",
    "* simple methods - Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using number of input (fan_in) and output (fan_out)\n",
    "W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update at 2015\n",
    "W = np.random.randn(fan_in, fan_out)/np.sqrt(fan_in/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2reg = 0.001 * tf.reduce_sum(tf.square(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout - prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* randomly set some nodes to zero in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = tf.placeholder(\"float\")\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "\n",
    "TRAIN:\n",
    "    sess.run(optimizer, feed_dict={X:batch_xs, Y:batch_ys, dropout_rate:0.7})  # 30 % dropout\n",
    "    \n",
    "EVALUATION:\n",
    "    accuracy.eval({X:test.images, Y:test.labels, dropout_rate:1})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
